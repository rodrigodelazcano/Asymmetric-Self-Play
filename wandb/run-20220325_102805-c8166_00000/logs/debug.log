2022-03-25 10:28:05,078 INFO    MainThread:226154 [wandb_setup.py:_flush():75] Loading settings from /home/rodri/.config/wandb/settings
2022-03-25 10:28:05,078 INFO    MainThread:226154 [wandb_setup.py:_flush():75] Loading settings from /home/rodri/Asymmetric-Self-Play-main/wandb/settings
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'api_key': '***REDACTED***', 'start_method': 'fork'}
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'trainer.py', 'program': 'trainer.py'}
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_init.py:_log_setup():405] Logging user logs to /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/logs/debug.log
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_init.py:_log_setup():406] Logging internal logs to /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/logs/debug-internal.log
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_init.py:init():439] calling init triggers
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_init.py:init():442] wandb.init called with sweep_config: {}
config: {'env': 'asym_self_play', 'num_workers': 1, 'num_envs_per_worker': 1, 'rollout_fragment_length': 1000, 'batch_mode': 'complete_episodes', 'framework': 'torch', 'train_batch_size': 2000, 'sgd_minibatch_size': 60, 'logger_config': {'wandb': {'project': 'Asymmetric_Self_Play', 'api_key': '1f77142634341e49c67a4f09fffb3bd79abc4f71'}}, 'multiagent': {'policies': {'alice_policy': "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True, '_time_major': False}, 'beta': None})", 'bob_policy': "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': False, '_time_major': False}, 'beta': 0.5})"}, 'policy_mapping_fn': '<function policy_mapping_fn at 0x7f0334ce5430>'}, 'sample_collector': "<class 'multi_episode_collector.MultiEpisodeCollector'>"}
2022-03-25 10:28:05,079 INFO    MainThread:226154 [wandb_init.py:init():492] starting backend
2022-03-25 10:28:05,079 INFO    MainThread:226154 [backend.py:_multiprocessing_setup():99] multiprocessing start_methods=fork,spawn,forkserver, using: fork
2022-03-25 10:28:05,080 INFO    MainThread:226154 [backend.py:ensure_launched():219] starting backend process...
2022-03-25 10:28:05,085 INFO    MainThread:226154 [backend.py:ensure_launched():224] started backend process with pid: 226199
2022-03-25 10:28:05,085 INFO    MainThread:226154 [wandb_init.py:init():501] backend started and connected
2022-03-25 10:28:05,091 INFO    MainThread:226154 [wandb_init.py:init():565] updated telemetry
2022-03-25 10:28:05,092 INFO    MainThread:226154 [wandb_init.py:init():596] communicating run to backend with 30 second timeout
2022-03-25 10:28:05,095 INFO    MainThread:226199 [internal.py:wandb_internal():92] W&B internal server running at pid: 226199, started at: 2022-03-25 10:28:05.094358
2022-03-25 10:28:05,097 INFO    WriterThread:226199 [datastore.py:open_for_write():77] open: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/run-c8166_00000.wandb
2022-03-25 10:28:05,105 INFO    SenderThread:226199 [sender.py:_maybe_setup_resume():492] checking resume status for None/Asymmetric_Self_Play/c8166_00000
2022-03-25 10:28:05,658 INFO    SenderThread:226199 [dir_watcher.py:__init__():169] watching files in: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files
2022-03-25 10:28:05,658 INFO    SenderThread:226199 [sender.py:_start_run_threads():812] run started: c8166_00000 with start time 1648218485
2022-03-25 10:28:05,660 INFO    MainThread:226154 [wandb_run.py:_on_init():1759] communicating current version
2022-03-25 10:28:05,660 INFO    SenderThread:226199 [sender.py:_save_file():947] saving file wandb-summary.json with policy end
2022-03-25 10:28:05,970 INFO    MainThread:226154 [wandb_run.py:_on_init():1763] got version response 
2022-03-25 10:28:05,970 INFO    MainThread:226154 [wandb_init.py:init():625] starting run threads in backend
2022-03-25 10:28:06,660 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_created():217] file/dir created: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/requirements.txt
2022-03-25 10:28:06,660 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_created():217] file/dir created: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/conda-environment.yaml
2022-03-25 10:28:06,660 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_created():217] file/dir created: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/wandb-summary.json
2022-03-25 10:28:07,622 INFO    SenderThread:226199 [sender.py:_save_file():947] saving file wandb-metadata.json with policy now
2022-03-25 10:28:07,625 INFO    MainThread:226154 [wandb_run.py:_console_start():1733] atexit reg
2022-03-25 10:28:07,626 INFO    MainThread:226154 [wandb_run.py:_redirect():1606] redirect: SettingsConsole.REDIRECT
2022-03-25 10:28:07,626 INFO    MainThread:226154 [wandb_run.py:_redirect():1611] Redirecting console.
2022-03-25 10:28:07,627 INFO    MainThread:226154 [wandb_run.py:_redirect():1667] Redirects installed.
2022-03-25 10:28:07,627 INFO    MainThread:226154 [wandb_init.py:init():664] run started, returning control to user process
2022-03-25 10:28:07,659 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_modified():230] file/dir modified: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/conda-environment.yaml
2022-03-25 10:28:07,659 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_created():217] file/dir created: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/wandb-metadata.json
2022-03-25 10:28:07,659 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_created():217] file/dir created: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/output.log
2022-03-25 10:28:08,400 INFO    Thread-12 :226199 [upload_job.py:push():137] Uploaded file /tmp/tmp6xev7sf3wandb/2dmtkzl3-wandb-metadata.json
2022-03-25 10:28:35,665 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_modified():230] file/dir modified: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/config.yaml
2022-03-25 10:30:20,608 INFO    MainThread:226154 [wandb_run.py:_config_callback():992] config_cb None None {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 1000, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 2000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'asym_self_play', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {'num_workers': 1, 'num_envs_per_worker': 1, 'create_env_on_driver': False, 'rollout_fragment_length': 1000, 'batch_mode': 'complete_episodes', 'gamma': 0.99, 'lr': 5e-05, 'train_batch_size': 2000, 'model': {'_use_default_native_models': False, '_disable_preprocessor_api': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1}, 'optimizer': {}, 'horizon': None, 'soft_horizon': False, 'no_done_at_end': False, 'env': 'asym_self_play', 'observation_space': None, 'action_space': None, 'env_config': {}, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'env_task_fn': None, 'render_env': False, 'record_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'preprocessor_pref': 'deepmind', 'log_level': 'WARN', 'callbacks': "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", 'ignore_worker_failures': False, 'log_sys_usage': True, 'fake_sampler': False, 'framework': 'torch', 'eager_tracing': False, 'eager_max_retraces': 20, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_parallel_to_training': False, 'in_evaluation': False, 'evaluation_config': {}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': "<class 'multi_episode_collector.MultiEpisodeCollector'>", 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'alice_policy': "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True, '_time_major': False}, 'beta': None})", 'bob_policy': "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': False, '_time_major': False}, 'beta': 0.5})"}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': '<function policy_mapping_fn at 0x7f02300a54c0>', 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': {'wandb': {'project': 'Asymmetric_Self_Play', 'api_key': '1f77142634341e49c67a4f09fffb3bd79abc4f71'}}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 60, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1}, 'evaluation_num_workers': 0, 'custom_eval_function': None, 'always_attach_evaluation_results': False, 'sample_async': False, 'sample_collector': "<class 'multi_episode_collector.MultiEpisodeCollector'>", 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': False, 'metrics_episode_collection_timeout_s': 180, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_reporting': None, 'min_train_timesteps_per_reporting': None, 'min_sample_timesteps_per_reporting': 0, 'seed': None, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'placement_strategy': 'PACK', 'input': 'sampler', 'input_config': {}, 'actions_in_input_normalized': False, 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {'alice_policy': "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True, '_time_major': False}, 'beta': None})", 'bob_policy': "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': False, '_time_major': False}, 'beta': 0.5})"}, 'policy_map_capacity': 100, 'policy_map_cache': None, 'policy_mapping_fn': '<function policy_mapping_fn at 0x7f02300a54c0>', 'policies_to_train': None, 'observation_fn': None, 'replay_mode': 'independent', 'count_steps_by': 'env_steps'}, 'logger_config': {'wandb': {'project': 'Asymmetric_Self_Play', 'api_key': '1f77142634341e49c67a4f09fffb3bd79abc4f71'}}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': False, 'simple_optimizer': False, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': 0, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'use_critic': True, 'use_gae': True, 'lambda': 1.0, 'kl_coeff': 0.2, 'sgd_minibatch_size': 60, 'shuffle_sequences': True, 'num_sgd_iter': 30, 'lr_schedule': None, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'grad_clip': None, 'kl_target': 0.01, 'vf_share_layers': -1, 'trial_id': 'c8166_00000', 'experiment_id': 'f1f9e4acca5f4d048e26098036101761', 'date': '2022-03-25_10-30-20', 'pid': 225789, 'hostname': 'lapt', 'node_ip': '132.163.197.244'}
2022-03-25 10:30:20,620 INFO    SenderThread:226199 [sender.py:_save_file():947] saving file wandb-summary.json with policy end
2022-03-25 10:30:20,689 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_modified():230] file/dir modified: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/wandb-summary.json
2022-03-25 10:30:38,693 INFO    Thread-8  :226199 [dir_watcher.py:_on_file_modified():230] file/dir modified: /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_102805-c8166_00000/files/config.yaml
2022-03-25 10:31:04,192 WARNING MainThread:226199 [internal.py:wandb_internal():155] Internal process interrupt: 1
2022-03-25 10:31:04,346 WARNING MainThread:226199 [internal.py:wandb_internal():155] Internal process interrupt: 2
2022-03-25 10:31:04,346 ERROR   MainThread:226199 [internal.py:wandb_internal():158] Internal process interrupted.
