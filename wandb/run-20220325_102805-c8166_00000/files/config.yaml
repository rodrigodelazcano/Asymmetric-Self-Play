wandb_version: 1

_disable_action_flattening:
  desc: null
  value: false
_disable_execution_plan_api:
  desc: null
  value: false
_disable_preprocessor_api:
  desc: null
  value: false
_fake_gpus:
  desc: null
  value: false
_tf_policy_handles_more_than_one_loss:
  desc: null
  value: false
_wandb:
  desc: null
  value:
    cli_version: 0.12.11
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    python_version: 3.8.12
    start_time: 1648218485
    t:
      1:
      - 1
      - 5
      - 30
      3:
      - 13
      - 14
      - 16
      4: 3.8.12
      5: 0.12.11
      8:
      - 6
      - 9
action_space:
  desc: null
  value: null
actions_in_input_normalized:
  desc: null
  value: false
always_attach_evaluation_results:
  desc: null
  value: false
batch_mode:
  desc: null
  value: complete_episodes
clip_actions:
  desc: null
  value: false
clip_param:
  desc: null
  value: 0.3
clip_rewards:
  desc: null
  value: null
collect_metrics_timeout:
  desc: null
  value: -1
compress_observations:
  desc: null
  value: false
create_env_on_driver:
  desc: null
  value: false
custom_eval_function:
  desc: null
  value: null
custom_resources_per_worker:
  desc: null
  value: {}
date:
  desc: null
  value: 2022-03-25_10-30-20
eager_max_retraces:
  desc: null
  value: 20
eager_tracing:
  desc: null
  value: false
entropy_coeff:
  desc: null
  value: 0.0
entropy_coeff_schedule:
  desc: null
  value: null
env:
  desc: null
  value: asym_self_play
env_config:
  desc: null
  value: {}
env_task_fn:
  desc: null
  value: null
evaluation_config:
  desc: null
  value:
    _disable_action_flattening: false
    _disable_execution_plan_api: false
    _disable_preprocessor_api: false
    _fake_gpus: false
    _tf_policy_handles_more_than_one_loss: false
    action_space: null
    actions_in_input_normalized: false
    always_attach_evaluation_results: false
    batch_mode: complete_episodes
    callbacks: <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>
    clip_actions: false
    clip_param: 0.3
    clip_rewards: null
    collect_metrics_timeout: -1
    compress_observations: false
    create_env_on_driver: false
    custom_eval_function: null
    custom_resources_per_worker: {}
    eager_max_retraces: 20
    eager_tracing: false
    entropy_coeff: 0.0
    entropy_coeff_schedule: null
    env: asym_self_play
    env_config: {}
    env_task_fn: null
    evaluation_config: {}
    evaluation_duration: 10
    evaluation_duration_unit: episodes
    evaluation_interval: null
    evaluation_num_episodes: -1
    evaluation_num_workers: 0
    evaluation_parallel_to_training: false
    exploration_config:
      type: StochasticSampling
    explore: true
    extra_python_environs_for_driver: {}
    extra_python_environs_for_worker: {}
    fake_sampler: false
    framework: torch
    gamma: 0.99
    grad_clip: null
    horizon: null
    ignore_worker_failures: false
    in_evaluation: false
    input: sampler
    input_config: {}
    input_evaluation:
    - is
    - wis
    kl_coeff: 0.2
    kl_target: 0.01
    lambda: 1.0
    local_tf_session_args:
      inter_op_parallelism_threads: 8
      intra_op_parallelism_threads: 8
    log_level: WARN
    log_sys_usage: true
    logger_config:
      wandb:
        api_key: 1f77142634341e49c67a4f09fffb3bd79abc4f71
        project: Asymmetric_Self_Play
    lr: 5.0e-05
    lr_schedule: null
    metrics_episode_collection_timeout_s: 180
    metrics_num_episodes_for_smoothing: 100
    metrics_smoothing_episodes: -1
    min_iter_time_s: -1
    min_sample_timesteps_per_reporting: 0
    min_time_s_per_reporting: null
    min_train_timesteps_per_reporting: null
    model:
      _disable_preprocessor_api: false
      _time_major: false
      _use_default_native_models: false
      attention_dim: 64
      attention_head_dim: 32
      attention_init_gru_gate_bias: 2.0
      attention_memory_inference: 50
      attention_memory_training: 50
      attention_num_heads: 1
      attention_num_transformer_units: 1
      attention_position_wise_mlp_dim: 32
      attention_use_n_prev_actions: 0
      attention_use_n_prev_rewards: 0
      conv_activation: relu
      conv_filters: null
      custom_action_dist: null
      custom_model: null
      custom_model_config: {}
      custom_preprocessor: null
      dim: 84
      fcnet_activation: tanh
      fcnet_hiddens:
      - 256
      - 256
      framestack: true
      free_log_std: false
      grayscale: false
      lstm_cell_size: 256
      lstm_use_prev_action: false
      lstm_use_prev_action_reward: -1
      lstm_use_prev_reward: false
      max_seq_len: 20
      no_final_linear: false
      post_fcnet_activation: relu
      post_fcnet_hiddens: []
      use_attention: false
      use_lstm: false
      vf_share_layers: false
      zero_mean: true
    monitor: -1
    multiagent:
      count_steps_by: env_steps
      observation_fn: null
      policies:
        alice_policy: "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>,\
          \ observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
          \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf\
          \ inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf\
          \ -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\
          \ inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5\
          \ -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11\
          \ 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model',\
          \ 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256,\
          \ 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
          \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf\
          \ inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf\
          \ -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\
          \ inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5\
          \ -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20,\
          \ 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward':\
          \ True, '_time_major': False}, 'beta': None})"
        bob_policy: "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>,\
          \ observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
          \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf\
          \ inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf\
          \ inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf\
          \ inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf\
          \ inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5\
          \ -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)),\
          \ action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model':\
          \ 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs':\
          \ 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf\
          \ inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf\
          \ inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf\
          \ inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf\
          \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf\
          \ inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf\
          \ inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5\
          \ -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))},\
          \ 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True,\
          \ 'lstm_use_prev_reward': False, '_time_major': False}, 'beta': 0.5})"
      policies_to_train: null
      policy_map_cache: null
      policy_map_capacity: 100
      policy_mapping_fn: <function policy_mapping_fn at 0x7f02300a54c0>
      replay_mode: independent
    no_done_at_end: false
    normalize_actions: true
    num_cpus_for_driver: 1
    num_cpus_per_worker: 1
    num_envs_per_worker: 1
    num_gpus: 0
    num_gpus_per_worker: 0
    num_sgd_iter: 30
    num_workers: 1
    observation_filter: NoFilter
    observation_space: null
    optimizer: {}
    output: null
    output_compress_columns:
    - obs
    - new_obs
    output_max_file_size: 67108864
    placement_strategy: PACK
    postprocess_inputs: false
    preprocessor_pref: deepmind
    record_env: false
    remote_env_batch_wait_ms: 0
    remote_worker_envs: false
    render_env: false
    rollout_fragment_length: 1000
    sample_async: false
    sample_collector: <class 'multi_episode_collector.MultiEpisodeCollector'>
    seed: null
    sgd_minibatch_size: 60
    shuffle_buffer_size: 0
    shuffle_sequences: true
    simple_optimizer: false
    soft_horizon: false
    synchronize_filters: true
    tf_session_args:
      allow_soft_placement: true
      device_count:
        CPU: 1
      gpu_options:
        allow_growth: true
      inter_op_parallelism_threads: 2
      intra_op_parallelism_threads: 2
      log_device_placement: false
    timesteps_per_iteration: 0
    train_batch_size: 2000
    use_critic: true
    use_gae: true
    vf_clip_param: 10.0
    vf_loss_coeff: 1.0
    vf_share_layers: -1
evaluation_duration:
  desc: null
  value: 10
evaluation_duration_unit:
  desc: null
  value: episodes
evaluation_interval:
  desc: null
  value: null
evaluation_num_episodes:
  desc: null
  value: -1
evaluation_num_workers:
  desc: null
  value: 0
evaluation_parallel_to_training:
  desc: null
  value: false
experiment_id:
  desc: null
  value: f1f9e4acca5f4d048e26098036101761
exploration_config:
  desc: null
  value:
    type: StochasticSampling
explore:
  desc: null
  value: true
extra_python_environs_for_driver:
  desc: null
  value: {}
extra_python_environs_for_worker:
  desc: null
  value: {}
fake_sampler:
  desc: null
  value: false
framework:
  desc: null
  value: torch
gamma:
  desc: null
  value: 0.99
grad_clip:
  desc: null
  value: null
horizon:
  desc: null
  value: null
hostname:
  desc: null
  value: lapt
ignore_worker_failures:
  desc: null
  value: false
in_evaluation:
  desc: null
  value: false
input:
  desc: null
  value: sampler
input_config:
  desc: null
  value: {}
input_evaluation:
  desc: null
  value:
  - is
  - wis
kl_coeff:
  desc: null
  value: 0.2
kl_target:
  desc: null
  value: 0.01
lambda:
  desc: null
  value: 1.0
local_tf_session_args:
  desc: null
  value:
    inter_op_parallelism_threads: 8
    intra_op_parallelism_threads: 8
log_level:
  desc: null
  value: WARN
log_sys_usage:
  desc: null
  value: true
logger_config:
  desc: null
  value:
    wandb:
      api_key: 1f77142634341e49c67a4f09fffb3bd79abc4f71
      project: Asymmetric_Self_Play
lr:
  desc: null
  value: 5.0e-05
lr_schedule:
  desc: null
  value: null
metrics_episode_collection_timeout_s:
  desc: null
  value: 180
metrics_num_episodes_for_smoothing:
  desc: null
  value: 100
metrics_smoothing_episodes:
  desc: null
  value: -1
min_iter_time_s:
  desc: null
  value: -1
min_sample_timesteps_per_reporting:
  desc: null
  value: 0
min_time_s_per_reporting:
  desc: null
  value: null
min_train_timesteps_per_reporting:
  desc: null
  value: null
model:
  desc: null
  value:
    _disable_preprocessor_api: false
    _time_major: false
    _use_default_native_models: false
    attention_dim: 64
    attention_head_dim: 32
    attention_init_gru_gate_bias: 2.0
    attention_memory_inference: 50
    attention_memory_training: 50
    attention_num_heads: 1
    attention_num_transformer_units: 1
    attention_position_wise_mlp_dim: 32
    attention_use_n_prev_actions: 0
    attention_use_n_prev_rewards: 0
    conv_activation: relu
    conv_filters: null
    custom_action_dist: null
    custom_model: null
    custom_model_config: {}
    custom_preprocessor: null
    dim: 84
    fcnet_activation: tanh
    fcnet_hiddens:
    - 256
    - 256
    framestack: true
    free_log_std: false
    grayscale: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_action_reward: -1
    lstm_use_prev_reward: false
    max_seq_len: 20
    no_final_linear: false
    post_fcnet_activation: relu
    post_fcnet_hiddens: []
    use_attention: false
    use_lstm: false
    vf_share_layers: false
    zero_mean: true
monitor:
  desc: null
  value: -1
multiagent:
  desc: null
  value:
    count_steps_by: env_steps
    observation_fn: null
    policies:
      alice_policy: "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>,\
        \ observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
        \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf\
        \ -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\
        \ inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5\
        \ -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11\
        \ 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model',\
        \ 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256,\
        \ 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
        \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf\
        \ -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\
        \ inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5\
        \ -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size':\
        \ 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True, '_time_major':\
        \ False}, 'beta': None})"
      bob_policy: "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>,\
        \ observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
        \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf\
        \ inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf\
        \ inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5\
        \ -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11\
        \ 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model',\
        \ 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256,\
        \ 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf],\
        \ (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf\
        \ inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf\
        \ -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf\
        \ inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf\
        \ inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5\
        \ -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len':\
        \ 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward':\
        \ False, '_time_major': False}, 'beta': 0.5})"
    policies_to_train: null
    policy_map_cache: null
    policy_map_capacity: 100
    policy_mapping_fn: <function policy_mapping_fn at 0x7f02300a54c0>
    replay_mode: independent
no_done_at_end:
  desc: null
  value: false
node_ip:
  desc: null
  value: 132.163.197.244
normalize_actions:
  desc: null
  value: true
num_cpus_for_driver:
  desc: null
  value: 1
num_cpus_per_worker:
  desc: null
  value: 1
num_envs_per_worker:
  desc: null
  value: 1
num_gpus:
  desc: null
  value: 0
num_gpus_per_worker:
  desc: null
  value: 0
num_sgd_iter:
  desc: null
  value: 30
num_workers:
  desc: null
  value: 1
observation_filter:
  desc: null
  value: NoFilter
observation_space:
  desc: null
  value: null
optimizer:
  desc: null
  value: {}
output:
  desc: null
  value: null
output_compress_columns:
  desc: null
  value:
  - obs
  - new_obs
output_max_file_size:
  desc: null
  value: 67108864
pid:
  desc: null
  value: 225789
placement_strategy:
  desc: null
  value: PACK
postprocess_inputs:
  desc: null
  value: false
preprocessor_pref:
  desc: null
  value: deepmind
record_env:
  desc: null
  value: false
remote_env_batch_wait_ms:
  desc: null
  value: 0
remote_worker_envs:
  desc: null
  value: false
render_env:
  desc: null
  value: false
rollout_fragment_length:
  desc: null
  value: 1000
sample_async:
  desc: null
  value: false
sample_collector:
  desc: null
  value: <class 'multi_episode_collector.MultiEpisodeCollector'>
seed:
  desc: null
  value: null
sgd_minibatch_size:
  desc: null
  value: 60
shuffle_buffer_size:
  desc: null
  value: 0
shuffle_sequences:
  desc: null
  value: true
simple_optimizer:
  desc: null
  value: false
soft_horizon:
  desc: null
  value: false
synchronize_filters:
  desc: null
  value: true
tf_session_args:
  desc: null
  value:
    allow_soft_placement: true
    device_count:
      CPU: 1
    gpu_options:
      allow_growth: true
    inter_op_parallelism_threads: 2
    intra_op_parallelism_threads: 2
    log_device_placement: false
timesteps_per_iteration:
  desc: null
  value: 0
train_batch_size:
  desc: null
  value: 2000
trial_id:
  desc: null
  value: c8166_00000
use_critic:
  desc: null
  value: true
use_gae:
  desc: null
  value: true
vf_clip_param:
  desc: null
  value: 10.0
vf_loss_coeff:
  desc: null
  value: 1.0
vf_share_layers:
  desc: null
  value: -1
