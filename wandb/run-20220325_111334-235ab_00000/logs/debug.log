2022-03-25 11:13:34,977 INFO    MainThread:245105 [wandb_setup.py:_flush():75] Loading settings from /home/rodri/.config/wandb/settings
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_setup.py:_flush():75] Loading settings from /home/rodri/Asymmetric-Self-Play-main/wandb/settings
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_setup.py:_flush():75] Loading settings from environment variables: {'api_key': '***REDACTED***', 'start_method': 'fork'}
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_setup.py:_flush():75] Inferring run settings from compute environment: {'program_relpath': 'trainer.py', 'program': 'trainer.py'}
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_init.py:_log_setup():405] Logging user logs to /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_111334-235ab_00000/logs/debug.log
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_init.py:_log_setup():406] Logging internal logs to /home/rodri/Asymmetric-Self-Play-main/wandb/run-20220325_111334-235ab_00000/logs/debug-internal.log
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_init.py:init():439] calling init triggers
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_init.py:init():442] wandb.init called with sweep_config: {}
config: {'env': 'asym_self_play', 'num_workers': 1, 'num_envs_per_worker': 1, 'rollout_fragment_length': 1000, 'batch_mode': 'complete_episodes', 'framework': 'torch', 'train_batch_size': 2000, 'sgd_minibatch_size': 60, 'logger_config': {'wandb': {'project': 'Asymmetric_Self_Play', 'api_key': '1f77142634341e49c67a4f09fffb3bd79abc4f71'}}, 'multiagent': {'policies': {'alice_policy': "PolicySpec(policy_class=<class 'ray.rllib.agents.ppo.ppo_torch_policy.PPOTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf], (17,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': True, '_time_major': False}, 'beta': None})", 'bob_policy': "PolicySpec(policy_class=<class 'policies.bob_policy.BobTorchPolicy'>, observation_space=Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32)), action_space=MultiDiscrete([11 11 11 11 11 11]), config={'model': {'custom_model': 'asym_torch_model', 'custom_model_config': {'number_of_objects': 2, 'num_model_outputs': 256, 'dict_obs_space': Dict(gripper_pos:Box([-inf -inf -inf], [inf inf inf], (3,), float32), obj_0_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), obj_1_state:Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n inf inf inf inf inf inf inf inf inf inf inf], (29,), float32), robot_joint_pos:Box([-6.5 -6.5 -6.5 -6.5 -6.5 -6.5], [6.5 6.5 6.5 6.5 6.5 6.5], (6,), float32))}, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': False, '_time_major': False}, 'beta': 0.5})"}, 'policy_mapping_fn': '<function policy_mapping_fn at 0x7f00faf0d280>'}, 'sample_collector': "<class 'multi_episode_collector.MultiEpisodeCollector'>"}
2022-03-25 11:13:34,978 INFO    MainThread:245105 [wandb_init.py:init():492] starting backend
2022-03-25 11:13:34,978 INFO    MainThread:245105 [backend.py:_multiprocessing_setup():99] multiprocessing start_methods=fork,spawn,forkserver, using: fork
2022-03-25 11:13:34,979 INFO    MainThread:245105 [backend.py:ensure_launched():219] starting backend process...
